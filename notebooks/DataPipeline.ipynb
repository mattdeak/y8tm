{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "class FrameConverter:\n",
    "    def __init__(self, X_transforms=[], y_transforms=[], repeat_count=1, n_parallel=1):\n",
    "        self.filename_base = '/home/data/full/frame/{}{}.tfrecord'\n",
    "        self.X_transforms = X_transforms\n",
    "        self.y_transforms = y_transforms\n",
    "        self.repeat_count = repeat_count\n",
    "        self.n_parallel = n_parallel\n",
    "        \n",
    "        self.keys_to_features = {\n",
    "            'rgb': tf.FixedLenSequenceFeature([], tf.string, allow_missing=True),\n",
    "            'audio': tf.FixedLenSequenceFeature([], tf.string, allow_missing=True),\n",
    "        }\n",
    "        self.key_to_label = {\n",
    "            'labels': tf.VarLenFeature(tf.int64)\n",
    "        }\n",
    "        \n",
    "    def get_train_data(self, filename):\n",
    "        y, X = tf.parse_single_sequence_example(filename,\n",
    "                                                self.key_to_label,\n",
    "                                                self.keys_to_features)\n",
    "        # X is still bytes; convert to float\n",
    "        X['audio'] = tf.cast(tf.decode_raw(X['audio'], tf.uint8), tf.float32)\n",
    "        X['rgb'] = tf.cast(tf.decode_raw(X['rgb'], tf.uint8), tf.float32)\n",
    "\n",
    "        # now apply custom transformations\n",
    "        for transform in self.X_transforms:\n",
    "            X = transform(X)\n",
    "\n",
    "        y = tf.sparse_to_dense(y['labels'].values, [3862], 1)\n",
    "        for transform in self.y_transforms:\n",
    "            y = transform(y)\n",
    "        return X, y\n",
    "\n",
    "    def get_test_data(self, filename):\n",
    "        X = tf.parse_single_sequence_example(filename, None, self.keys_to_features)[1]\n",
    "        \n",
    "        # X is still bytes; convert to float\n",
    "        X['audio'] = tf.cast(tf.decode_raw(X['audio'], tf.uint8), tf.float32)\n",
    "        X['rgb'] = tf.cast(tf.decode_raw(X['rgb'], tf.uint8), tf.float32)\n",
    "\n",
    "        # now apply custom transformations\n",
    "        for transform in self.X_transforms:\n",
    "            X = transform(X)\n",
    "        return X\n",
    "    \n",
    "    def make_provider(self, subset, record_indices):\n",
    "        filenames = [self.filename_base.format(subset, index) for index in record_indices]\n",
    "        \n",
    "        dataset = tf.data.TFRecordDataset(filenames)\n",
    "        \n",
    "        if subset == 'train':\n",
    "            dataset = dataset.map(self.get_train_data,\n",
    "                                  num_parallel_calls=self.n_parallel)\n",
    "            dataset = dataset.repeat(self.repeat_count)\n",
    "            dataset = dataset.shuffle(buffer_size=256)\n",
    "        else:\n",
    "            dataset = dataset.map(self.get_test_data,\n",
    "                                  num_parallel_calls=self.n_parallel)\n",
    "        dataset = dataset.batch(1)\n",
    "        dataset = dataset.prefetch(1)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        return iterator\n",
    "    \n",
    "    def make_generator(self, subset, record_indices):\n",
    "        provider = self.make_provider(subset, record_indices)\n",
    "        sess = tf.Session()\n",
    "        next_sample = provider.get_next()\n",
    "        while True:\n",
    "            try:\n",
    "                yield sess.run(next_sample)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Iterations exhausted\")\n",
    "                break\n",
    "    \n",
    "frame_converter = FrameConverter()\n",
    "train_generator = frame_converter.make_generator('train', [2500])\n",
    "valid_generator = frame_converter.make_generator('validate', [2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'audio': array([[[173.,  27., 126., ..., 133., 130., 187.],\n",
       "          [139.,  36., 100., ..., 107.,  98.,   0.],\n",
       "          [138.,  65., 101., ...,  84.,  77., 231.],\n",
       "          ...,\n",
       "          [158.,  35., 123., ..., 103.,  87., 223.],\n",
       "          [173.,  27., 126., ..., 133., 130., 187.],\n",
       "          [173.,  27., 126., ..., 133., 130., 187.]]], dtype=float32),\n",
       "  'rgb': array([[[ 72.,  96., 129., ..., 128.,  85., 190.],\n",
       "          [ 72., 102., 130., ..., 127.,  84., 186.],\n",
       "          [  0.,  59., 184., ..., 197., 105., 119.],\n",
       "          ...,\n",
       "          [176., 115., 117., ...,  26., 189., 166.],\n",
       "          [ 77.,  96., 131., ..., 124.,  85., 173.],\n",
       "          [ 76.,  96., 131., ..., 126.,  88., 176.]]], dtype=float32)},\n",
       " array([[0, 0, 1, ..., 0, 0, 0]], dtype=int32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Input, Dense, GRU, Flatten, Add\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "n_classes = 3862\n",
    "rgb_in = Input((None, 1024), name='rgb')\n",
    "audio_in = Input((None, 128), name='audio')\n",
    "rgb_mid = GRU(64, activation='relu')(rgb_in)\n",
    "audio_mid = GRU(64, activation='relu')(audio_in)\n",
    "combined_mid = Add()([rgb_mid, audio_mid])\n",
    "out = Dense(32, activation='relu')(combined_mid)\n",
    "out = Dense(n_classes, activation='softmax')(out)\n",
    "model = Model([rgb_in, audio_in], out)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "29/30 [============================>.] - ETA: 0s - loss: 0.0159 - acc: 0.9989"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0f7d63d78f65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit_generator(train_generator, steps_per_epoch=30, epochs=1,\n\u001b[1;32m      2\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     validation_steps=20)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1596\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    210\u001b[0m                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                 max_queue_size=max_queue_size)\n\u001b[0m\u001b[1;32m    213\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    309\u001b[0m                          \u001b[0;34m'(x, y, sample_weight) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                          'or (x, y). Found: ' + str(generator_output))\n\u001b[0;32m--> 311\u001b[0;31m       \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \"\"\"\n\u001b[1;32m   1427\u001b[0m     x, y, sample_weights = self._standardize_user_data(\n\u001b[0;32m-> 1428\u001b[0;31m         x, y, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m   1429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/engine/training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DataFrame'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstandardize_single_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/_impl/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_single_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m   \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=30, epochs=1,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3862)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "model.predict([np.random.random((10, 300, 1024)), np.random.random((10, 300, 128))]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
